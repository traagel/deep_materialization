{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94a59ee",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will be building a model to predict drug interactions on the [ogbl-ddi](https://ogb.stanford.edu/docs/linkprop/#ogbl-ddi) drug-drug interaction network and trying to match performance to the OGB Leaderboard for Link Property Prediction. Once we can re-create this using a baseline GraphSAGE architecture, we will substantially increase performance by adding [Jumping Knowledge](https://arxiv.org/abs/1806.03536) connections.\n",
    "\n",
    "## What you need to do\n",
    "### Basics\n",
    "\n",
    "We'll start by implementing the GNN encoder model by stacking GraphSAGE layers to create node embeddings.  We'll then implement the Link Prediction decoder model, which takes two node embeddings as input and predicts whether or not those two nodes share an edge in the graph.  Sections you need to write code for will be marked with a \"TODO\" comment. For ease of troubleshooting, do not modify any of the code except where indicated until after the basics have been achieved.  We will also import functionality from the `utils.py` script.  No changes should be necessary to this file.\n",
    "\n",
    "### Improvements\n",
    "\n",
    "Once you have a correct implementation and can match leaderboard performance, we can move on to experimentation. In this section, we will implement Jumping Knowledge connections, which allows the GNN model to jointly consider information from multiple distances in the graph and gives substantial improvements above baseline GraphSAGE.\n",
    "\n",
    "### Statistics\n",
    "\n",
    "In each section, the `repeat_experiments` function is used to run the same model building process multiple times to collect statistics since there is significant variation in performance from run-to-run.  This allows us to make apples-to-apples comparisons to the leaderboard, but this process takes significant time.  Feel free to reduce from the default setup of 10 experiments with 200 epochs each to something much smaller (e.g., 1 experiment with 10 epochs) until you're confident in the implementation.\n",
    "\n",
    "\n",
    "### Extra Credit\n",
    "\n",
    "A great way to learn is to 1) read other peoples' code and 2) tinker. If you want to maximize your learning, the Extra Credit section will provide a few directions for further exploration. As part of this, it's recommended you find another submission on the OGB Leaderboard and try to re-create its submission. This will likely involve reading the paper and code, which is great practice. If you do something interesting and novel, we'll kindly host it in a public repo as an example of your glory (if you wish).\n",
    "\n",
    "\n",
    "## Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d166bd70",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a52c68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c16c7c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.13.1+cpu.  CUDA version: None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Torch version: {th.__version__}.  CUDA version: {th.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "699f0e4b",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.dgl.ai/wheels/repo.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement dgl-cu101 (from versions: none)\n",
      "ERROR: No matching distribution found for dgl-cu101\n"
     ]
    }
   ],
   "source": [
    "# Follow instructions at https://www.dgl.ai/pages/start.html\n",
    "!{sys.executable} -m pip install dgl-cu101 -f https://data.dgl.ai/wheels/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62fdca27",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ogb\n",
      "  Downloading ogb-1.3.5-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.6/78.6 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\martt\\anaconda3\\envs\\deep_materialization\\lib\\site-packages (from ogb) (1.5.3)\n",
      "Collecting outdated>=0.2.0\n",
      "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting scikit-learn>=0.20.0\n",
      "  Downloading scikit_learn-1.2.0-cp310-cp310-win_amd64.whl (8.2 MB)\n",
      "     ---------------------------------------- 8.2/8.2 MB 17.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\martt\\anaconda3\\envs\\deep_materialization\\lib\\site-packages (from ogb) (1.24.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\martt\\anaconda3\\envs\\deep_materialization\\lib\\site-packages (from ogb) (1.13.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\martt\\anaconda3\\envs\\deep_materialization\\lib\\site-packages (from ogb) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in c:\\users\\martt\\anaconda3\\envs\\deep_materialization\\lib\\site-packages (from ogb) (1.26.14)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in c:\\users\\martt\\anaconda3\\envs\\deep_materialization\\lib\\site-packages (from ogb) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\martt\\anaconda3\\envs\\deep_materialization\\lib\\site-packages (from outdated>=0.2.0->ogb) (2.28.1)\n",
      "Collecting littleutils\n",
      "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: setuptools>=44 in c:\\users\\martt\\anaconda3\\envs\\deep_materialization\\lib\\site-packages (from outdated>=0.2.0->ogb) (65.6.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\martt\\anaconda3\\envs\\deep_materialization\\lib\\site-packages (from pandas>=0.24.0->ogb) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\martt\\anaconda3\\envs\\deep_materialization\\lib\\site-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\martt\\anaconda3\\envs\\deep_materialization\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (1.10.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     ------------------------------------- 298.0/298.0 kB 18.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\martt\\anaconda3\\envs\\deep_materialization\\lib\\site-packages (from torch>=1.6.0->ogb) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\martt\\anaconda3\\envs\\deep_materialization\\lib\\site-packages (from tqdm>=4.29.0->ogb) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\martt\\anaconda3\\envs\\deep_materialization\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\martt\\anaconda3\\envs\\deep_materialization\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\martt\\anaconda3\\envs\\deep_materialization\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
      "Building wheels for collected packages: littleutils\n",
      "  Building wheel for littleutils (setup.py): started\n",
      "  Building wheel for littleutils (setup.py): finished with status 'done'\n",
      "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7028 sha256=368557f52d78d2a046fb913f84b5ca613ee0f9f4741d03e6743dd485e0d4f8fd\n",
      "  Stored in directory: c:\\users\\martt\\appdata\\local\\pip\\cache\\wheels\\e0\\3b\\9c\\d55ff5bc6cfbe70537c4731a22f2ee2462c2e5010b56ac9726\n",
      "Successfully built littleutils\n",
      "Installing collected packages: littleutils, threadpoolctl, joblib, scikit-learn, outdated, ogb\n",
      "Successfully installed joblib-1.2.0 littleutils-0.2.2 ogb-1.3.5 outdated-0.2.2 scikit-learn-1.2.0 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "682318a5",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "os.environ['DGLBACKEND'] = 'pytorch'\n",
    "import dgl\n",
    "from dgl.nn import SAGEConv, GraphConv\n",
    "from dgl.dataloading import negative_sampler\n",
    "import dgl.function as fn\n",
    "from ogb.linkproppred import DglLinkPropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "762bb439",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd4a44e4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if th.cuda.is_available() else 'cpu'\n",
    "device = th.device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31fbd68",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc96a329",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ceb42a2",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from utils import repeat_experiments, SAGE, norm_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c3f476",
   "metadata": {},
   "source": [
    "# Basics\n",
    "## Get OGB dataset and GraphSAGE leaderboard performance\n",
    "We will use the `ogb` python package to download the data, give us the correct train/validation/test splits for proper comparison and an `Evaluator` to make sure performance is measured appropriately.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69c7d040",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://snap.stanford.edu/ogb/data/linkproppred/ddi.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.04 GB: 100%|██████████| 46/46 [00:14<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/ddi.zip\n",
      "Loading necessary files...\n",
      "This might take a while.\n",
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 49.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into DGL objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 166.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = DglLinkPropPredDataset(name=\"ogbl-ddi\", root = 'dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef6c8b1d",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Values from the OGB leaderboard of GraphSAGE, submitted June 25, 2020\n",
    "# HITS@20:  VAL = 0.6262 ± 0.0037;  TEST = 0.5390 ± 0.0474\n",
    "val_hits_lb, val_hits_lb_std, test_hits_lb, test_hits_lb_std = 0.6262, 0.0037, 0.5390, 0.0474\n",
    "evaluator = Evaluator(name = \"ogbl-ddi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44861890",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Extract split indices, DGL graph and labels\n",
    "split_idx = dataset.get_edge_split()\n",
    "g = dataset.graph[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c6183eb",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "src_orig, dst_orig = g.edges()\n",
    "src_orig = src_orig.clone()\n",
    "dst_orig = dst_orig.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18f68d98",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Use a portion of the training edges for periodic evaluation\n",
    "th.manual_seed(12345)\n",
    "idx = th.randperm(split_idx['train']['edge'].size(0))\n",
    "idx = idx[:split_idx['valid']['edge'].size(0)]\n",
    "split_idx['eval_train'] = {'edge': split_idx['train']['edge'][idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0589994f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e235011",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dataset.meta_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a788fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure none of the validation/test edges are in the graph used for training\n",
    "assert g.has_edges_between(split_idx['valid']['edge'][:,0], split_idx['valid']['edge'][:,1]).sum().item()==0\n",
    "assert g.has_edges_between(split_idx['test']['edge'][:,0], split_idx['test']['edge'][:,1]).sum().item()==0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0be27a9",
   "metadata": {},
   "source": [
    "## Approach overview\n",
    "We will use a GNN for creating node embeddings, and then use a final Link Predictor model that takes a pair of embeddings as input and returns a prediction for whether or not those two nodes share an edge. \n",
    "\n",
    "We are going to train our model end-to-end by having it distinguish between true edges found in our training graph (i.e., \"positive samples\") and fake edges we generate by pairing random nodes (i.e., \"negative samples\").  We treat this \"real vs fake\" edge prediction task as a binary classification problem by giving the true edges a $y=1$ label and fake edges a $y=0$ label.  This is known as \"contrastive loss\".  For each minibatch, the training loop is like this:\n",
    "\n",
    "- Calculate node embeddings for every node using the current GNN model parameters\n",
    "  - This is standard message passing on the graph using the *training edges only*.  No edges from the validation/test set or fake edges are part of this message passing graph.\n",
    "- Randomly sample a set of training edges (i.e., pairs of nodes in the training set) to construct the _positive samples_ and generate fake edges by randomly pairing nodes in the graph to construct the _negative samples_.  \n",
    "  - In both cases, an edge sample is a pair of nodes like `(source_nodeId, destination_nodeId)`.\n",
    "- Look-up the node embeddings for each of the source and destination nodes in the positive and negative samples\n",
    "- Using the node embedding pairs and Link Predictor model, generate predictions for both the positive and negative edges: `pos_out` and `neg_out`, which represent probabilities that the node pairs have an edge in the graph\n",
    "- Calculate a Binary Cross-Entropy loss where the positive edges have a label of 1 and negative a label of 0: \n",
    "\n",
    "$$ loss = -th.log(pos\\_out).mean() - th.log(1 - neg\\_out).mean() $$\n",
    "\n",
    "\n",
    "In the above, I emphasize that the message passing graph is distinct from the edge-set we use for supervision and evaluation with the Link Predictor.  More specifically, the message passing graph uses the positive training edges only and does not have access to the validation/test edges nor does it use the negative edges for message passing.  The message passing graph is just used to calculate embeddings for the nodes.  The edge-set used for supervision and evaluation are a distinct concept: these are the node pairings sent to the Link Predictor and have implicit labels of \"real or fake\" that we can use for training and evaluation.  \n",
    "\n",
    "Note that it's a form of information leak if you include an edge in message passing that you're also trying to use for prediction/supervision.  In a real-life scenario where you're trying to predict new edges, you will not have that edge as part of the graph to use in a message passing operation.\n",
    "\n",
    "\n",
    "### Data description\n",
    "This dataset consists of a homogeneous graph of 4k drug nodes and 1.3M edges that represent interactions in which taking the two drugs together gives a substantially different effect than taking them independently.  The edges are split into train, validation and test based on the proteins the drugs are designed to target in the body.  This means the drugs in the test set work differently than those in training and validation.  More details on the particulars of the dataset can be found on the [OGB dataset page](https://ogb.stanford.edu/docs/linkprop/#ogbl-ddi).\n",
    "\n",
    "## Model\n",
    "### Node Features\n",
    "In this case our drug nodes do not have natural features and we use 256-dimensional learnable features for each node.  These are input to the GNN to calculate embeddings and are jointly learned with the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b509ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 256\n",
    "emb = nn.Embedding(g.num_nodes(), num_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0d62b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(p.numel() for p in emb.parameters()) == 1092352, \"Number of embedding parameters doesn't match\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbcd341",
   "metadata": {},
   "source": [
    "### GNN model architecture\n",
    "Using the learnable features as input, the GNN model is a 2-layered GraphSAGE architecture where each layer $k$ updates the embedding $\\mathbf{h}$ for node $i$ using the following equation:\n",
    "\n",
    "$$\\mathbf{h}^{(k+1)}_i = \\sigma \\left ( \\mathbf{W}^{(k)}\\text{concat} \\left ( \\mathbf{h}_i^{(k)}, \\frac{1}{\\left | \\mathcal{N}_i \\right |}\\sum_{j \\in \\mathcal{N}_i}\\mathbf{h}_{j}^{(k)}  \\right ) \\right )$$\n",
    "\n",
    "This averages the neighborhood features $\\frac{1}{\\left | \\mathcal{N}_i \\right |}\\sum_{j \\in \\mathcal{N}_i}\\mathbf{h}_{j}^{(k)}$, concatenates to the current destination node embedding $\\mathbf{h}_{i}^{(k)}$, applies a linear projection and finally a non-linear activation function. \n",
    "\n",
    "Interestingly, it can be shown that concatenating two vectors and applying a linear projection is equal to applying separate linear projections to each vector and adding the results.  This is demonstrated if you mentally carry out the matrix multiplication operations:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\leftarrow  \\mathbf{h}_0  \\rightarrow & \\leftarrow  \\mathbf{h}_{\\mathcal{N}(0)}  \\rightarrow \\\\\n",
    "\\leftarrow  \\mathbf{h}_1  \\rightarrow & \\leftarrow  \\mathbf{h}_{\\mathcal{N}(1)}  \\rightarrow \n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "\\uparrow   &  \\uparrow & & \\uparrow \\\\\n",
    "\\mathbf{W}_0^h & \\mathbf{W}_1^h  & \\dots & \\mathbf{W}_H^h \\\\\n",
    "\\downarrow & \\downarrow & & \\downarrow \\\\\n",
    "\\uparrow   &  \\uparrow & & \\uparrow \\\\\n",
    "\\mathbf{W}_0^\\mathcal{N} & \\mathbf{W}_{1}^\\mathcal{N}  & \\dots & \\mathbf{W}_{H}^\\mathcal{N} \\\\\n",
    "\\downarrow & \\downarrow & & \\downarrow \n",
    "\\end{bmatrix} \n",
    "= \\mathbf{H} \\mathbf{W}^h + \\mathbf{H}_\\mathcal{N}\\mathbf{W}^\\mathcal{N}$$\n",
    "\n",
    "In [PyG](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/sage_conv.html#SAGEConv) and [DGL](https://docs.dgl.ai/_modules/dgl/nn/pytorch/conv/sageconv.html#SAGEConv) implementations of GraphSAGE you see this second form of adding the result of separate linear projections:\n",
    "\n",
    "$$ \\mathbf{h}^{(k+1)}_i = \\sigma \\left ( \\mathbf{W}^{(k)}_{self} \\mathbf{h}_i^{(k)} + \\mathbf{W}^{(k)}_{rel} \\frac{1}{\\left | \\mathcal{N}_i \\right |}\\sum_{j \\in \\mathcal{N}_i}\\mathbf{h}_{j}^{(k)} + \\mathbf{b}^{(k)} \\right )$$\n",
    "\n",
    "Note that in the above equation I also added a bias term $\\mathbf{b}$, which is commonly included.  We will implement this final form with the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87920788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySage(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements a single GraphSAGE layer as given by \n",
    "    the final equation above\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MySage, self).__init__()\n",
    "        # TODO\n",
    "        \"\"\"\n",
    "        Create all the parameters in the final equation above.  Don't forget \n",
    "        to include the bias term.\n",
    "        \n",
    "        \n",
    "        Arguments\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of input features to the layer\n",
    "        out_channels : int\n",
    "            Number of desired output features from the layer\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # TODO\n",
    "        \"\"\"\n",
    "        A method that defines how to re-initialize all the parameters of\n",
    "        your model.  This method will be invoked to start a new experiment,\n",
    "        so it is important that all parameters are reset, or else there will \n",
    "        be an information leak between experiments.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def forward(self, g, x):\n",
    "        # TODO\n",
    "        \"\"\"\n",
    "        The forward pass of the model, which applies all of the layers\n",
    "        according to the equation to a given graph and set of node features\n",
    "        \n",
    "        Note:  do NOT apply the final non-linearity in this method.  The\n",
    "        appropriate non-linearity will be applied to the output of this class.\n",
    "        This method should implement:\n",
    "        \n",
    "            W_self*h_i + W_neigh*h_neigh + b\n",
    "        \n",
    "        Arguments\n",
    "        ----------\n",
    "        g : DGLGraph\n",
    "            The graph used for message passing\n",
    "        x : Tensor\n",
    "            The node features\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816bb949",
   "metadata": {},
   "source": [
    "Your class will be passed as an argument to the `SAGE` wrapper class, defined in the `utils.py` script.  This class merely stacks together two layers of the class you have defined and adds the appropriate non-linearity, dropout...etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77cad28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "model = SAGE(\n",
    "    num_hidden, num_hidden, num_hidden, num_layers,\n",
    "    dropout, sage_cls=MySage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b59c3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(p.numel() for p in model.parameters()) == 262656, \"Number of GNN model parameters doesn't match\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57d07b2",
   "metadata": {},
   "source": [
    "### Link Predictor architecture\n",
    "Next you will implement a Link Predictor that takes two node embeddings as input and calculates an edge embedding $ \\mathbf{e}_{u,v} $ via an element-wise product of the node embeddings:  \n",
    "\n",
    "$$ \\mathbf{e}_{u,v} = \\mathbf{h}_u \\odot \\mathbf{h}_v$$\n",
    "\n",
    "It then passes that result through a 2-layered dense Neural Network.\n",
    "\n",
    "$$ \\hat{y}_{u,v} = \\text{sigmoid} \\left( \\mathbf{W}^1 \\text{ReLU} \\left(\\mathbf{W}^0 \\mathbf{e}_{u,v} + \\mathbf{b}^0 \\right) + \\mathbf{b}^1 \\right)$$\n",
    "\n",
    "This output represents our predicted probability that two nodes share an edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb27d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements a Link Predictor model\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, \n",
    "                 num_layers, dropout):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        # TODO\n",
    "        \"\"\"\n",
    "        Define the Link Predictor architectural components.  Note that \n",
    "        dropout needs to be applied to the hidden layer representations\n",
    "        \n",
    "        \n",
    "        Arguments\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of input features to the layer\n",
    "        hidden_channels : int\n",
    "            Number of channels in the hidden layers\n",
    "        out_channels : int\n",
    "            Number of desired output features from the layer\n",
    "        num_layers : int\n",
    "            Number of dense neural network layers to apply to\n",
    "            the edge embedding\n",
    "        dropout : float\n",
    "            Amount of dropout to apply to the hidden layers\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # TODO\n",
    "        \"\"\"\n",
    "        A method that defines how to re-initialize all the parameters of\n",
    "        your model.  This method will be invoked to start a new experiment,\n",
    "        so it is important that all parameters are reset, or else there will \n",
    "        be an information leak between experiments.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        # TODO\n",
    "        \"\"\"\n",
    "        The forward pass of the model, which calculates the edge embedding\n",
    "        from the two node embeddings and applies the dense neural network\n",
    "        layers.  This method will also implement the relevant non-linearities\n",
    "        and dropout.\n",
    "        \n",
    "        Each hidden layer of the NN should have the flow:\n",
    "            input -> linear -> ReLU -> Dropout\n",
    "        \n",
    "        The final layer of the NN should have the flow:\n",
    "            input -> linear -> Sigmoid\n",
    "            \n",
    "        Note that there is no dropout applied to the final layer.\n",
    "        \n",
    "        Arguments\n",
    "        ----------\n",
    "        x_i : Tensor\n",
    "            The source nodes' embeddings\n",
    "        x_j : Tensor\n",
    "            The destination nodes' embeddings\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dedcb189",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = LinkPredictor(\n",
    "    num_hidden, num_hidden, 1,\n",
    "    num_layers, dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e2116be",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(p.numel() for p in predictor.parameters()) == 66049, \"Number of Link Predictor parameters doesn't match\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e13f15",
   "metadata": {},
   "source": [
    "## Training\n",
    "This will leverage functions defined in the `utils.py` script to standardize training and evaluation.  \n",
    "\n",
    "For faster troubleshooting, note that you should be able to get close to the following performance within the first 10 training epochs, although it varies significantly from run to run:\n",
    "```\n",
    "Hits@10\n",
    "Run: 01, Epoch: 10, Loss: 0.4170, Train: 21.47%, Valid: 18.70%, Test: 7.49%\n",
    "Hits@20\n",
    "Run: 01, Epoch: 10, Loss: 0.4170, Train: 28.53%, Valid: 25.24%, Test: 12.05%\n",
    "Hits@30\n",
    "Run: 01, Epoch: 10, Loss: 0.4170, Train: 31.95%, Valid: 28.63%, Test: 16.94%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a88ce8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: reduce this to something like 1 run for 10 epochs when developing\n",
    "# and use 10 runs at 200 epochs for the final run\n",
    "N_runs = 10\n",
    "train_args = dict(epochs=200, lr=0.005, eval_steps=1, log_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92f9f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@10\n",
      "Run: 01, Epoch: 01, Loss: 1.1164, Train: 1.99%, Valid: 1.94%, Test: 3.31%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 01, Loss: 1.1164, Train: 3.05%, Valid: 2.85%, Test: 4.55%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 01, Loss: 1.1164, Train: 3.75%, Valid: 3.48%, Test: 5.26%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 02, Loss: 0.7831, Train: 2.16%, Valid: 1.88%, Test: 0.11%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 02, Loss: 0.7831, Train: 6.15%, Valid: 5.62%, Test: 3.39%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 02, Loss: 0.7831, Train: 7.50%, Valid: 6.84%, Test: 4.77%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 03, Loss: 0.6958, Train: 1.07%, Valid: 0.91%, Test: 0.39%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 03, Loss: 0.6958, Train: 7.67%, Valid: 6.96%, Test: 6.00%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 03, Loss: 0.6958, Train: 9.31%, Valid: 8.46%, Test: 8.38%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 04, Loss: 0.6230, Train: 4.81%, Valid: 4.19%, Test: 0.68%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 04, Loss: 0.6230, Train: 7.03%, Valid: 6.18%, Test: 4.87%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 04, Loss: 0.6230, Train: 8.96%, Valid: 7.93%, Test: 11.10%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 05, Loss: 0.5798, Train: 9.46%, Valid: 8.39%, Test: 5.87%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 05, Loss: 0.5798, Train: 12.11%, Valid: 10.71%, Test: 11.82%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 05, Loss: 0.5798, Train: 14.66%, Valid: 13.07%, Test: 14.74%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 06, Loss: 0.5231, Train: 18.92%, Valid: 16.89%, Test: 4.49%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 06, Loss: 0.5231, Train: 22.82%, Valid: 20.47%, Test: 9.70%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 06, Loss: 0.5231, Train: 25.82%, Valid: 23.37%, Test: 14.10%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 07, Loss: 0.4800, Train: 13.83%, Valid: 12.19%, Test: 3.24%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 07, Loss: 0.4800, Train: 18.34%, Valid: 16.30%, Test: 6.36%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 07, Loss: 0.4800, Train: 21.85%, Valid: 19.34%, Test: 11.24%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 08, Loss: 0.4488, Train: 22.45%, Valid: 19.79%, Test: 4.71%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 08, Loss: 0.4488, Train: 26.17%, Valid: 23.22%, Test: 9.38%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 08, Loss: 0.4488, Train: 29.02%, Valid: 25.96%, Test: 14.19%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 09, Loss: 0.4193, Train: 24.79%, Valid: 21.68%, Test: 6.99%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 09, Loss: 0.4193, Train: 31.68%, Valid: 28.33%, Test: 12.65%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 09, Loss: 0.4193, Train: 34.70%, Valid: 31.19%, Test: 15.92%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 10, Loss: 0.3951, Train: 28.63%, Valid: 25.22%, Test: 7.79%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 10, Loss: 0.3951, Train: 31.64%, Valid: 28.11%, Test: 12.80%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 10, Loss: 0.3951, Train: 37.11%, Valid: 33.29%, Test: 16.40%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 11, Loss: 0.3738, Train: 27.38%, Valid: 23.90%, Test: 5.23%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 11, Loss: 0.3738, Train: 33.33%, Valid: 29.32%, Test: 10.85%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 11, Loss: 0.3738, Train: 38.17%, Valid: 33.80%, Test: 13.98%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 12, Loss: 0.3571, Train: 24.75%, Valid: 21.24%, Test: 5.12%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 12, Loss: 0.3571, Train: 33.56%, Valid: 29.28%, Test: 8.03%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 12, Loss: 0.3571, Train: 36.18%, Valid: 31.77%, Test: 13.90%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 13, Loss: 0.3422, Train: 24.38%, Valid: 20.99%, Test: 6.48%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 13, Loss: 0.3422, Train: 31.95%, Valid: 27.80%, Test: 10.77%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 13, Loss: 0.3422, Train: 37.24%, Valid: 32.62%, Test: 15.92%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 14, Loss: 0.3293, Train: 37.94%, Valid: 33.12%, Test: 10.10%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 14, Loss: 0.3293, Train: 42.75%, Valid: 37.52%, Test: 15.09%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 14, Loss: 0.3293, Train: 44.42%, Valid: 39.09%, Test: 20.36%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 15, Loss: 0.3183, Train: 25.42%, Valid: 21.62%, Test: 7.84%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 15, Loss: 0.3183, Train: 33.83%, Valid: 28.98%, Test: 12.47%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 15, Loss: 0.3183, Train: 41.42%, Valid: 35.82%, Test: 16.99%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 16, Loss: 0.3086, Train: 37.41%, Valid: 32.38%, Test: 12.23%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 16, Loss: 0.3086, Train: 43.85%, Valid: 38.35%, Test: 18.68%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 16, Loss: 0.3086, Train: 50.81%, Valid: 44.97%, Test: 27.40%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 17, Loss: 0.3019, Train: 37.84%, Valid: 32.72%, Test: 11.19%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 17, Loss: 0.3019, Train: 47.91%, Valid: 42.00%, Test: 16.90%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 17, Loss: 0.3019, Train: 50.12%, Valid: 44.10%, Test: 22.42%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 18, Loss: 0.2933, Train: 36.93%, Valid: 31.75%, Test: 6.62%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 18, Loss: 0.2933, Train: 43.59%, Valid: 37.91%, Test: 11.48%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 18, Loss: 0.2933, Train: 48.50%, Valid: 42.44%, Test: 17.94%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 19, Loss: 0.2873, Train: 42.39%, Valid: 36.59%, Test: 14.84%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 19, Loss: 0.2873, Train: 48.13%, Valid: 41.85%, Test: 23.18%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 19, Loss: 0.2873, Train: 50.79%, Valid: 44.39%, Test: 27.05%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 20, Loss: 0.2823, Train: 27.02%, Valid: 22.69%, Test: 11.51%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 20, Loss: 0.2823, Train: 38.83%, Valid: 33.23%, Test: 19.23%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 20, Loss: 0.2823, Train: 45.70%, Valid: 39.49%, Test: 22.59%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 21, Loss: 0.2761, Train: 42.67%, Valid: 36.75%, Test: 22.86%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 21, Loss: 0.2761, Train: 49.70%, Valid: 43.28%, Test: 26.69%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 21, Loss: 0.2761, Train: 52.95%, Valid: 46.29%, Test: 31.58%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 22, Loss: 0.2685, Train: 44.66%, Valid: 38.29%, Test: 12.96%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 22, Loss: 0.2685, Train: 49.60%, Valid: 42.89%, Test: 19.51%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 22, Loss: 0.2685, Train: 53.64%, Valid: 46.80%, Test: 24.57%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 23, Loss: 0.2642, Train: 41.18%, Valid: 35.14%, Test: 15.06%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 23, Loss: 0.2642, Train: 49.62%, Valid: 42.78%, Test: 23.00%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 23, Loss: 0.2642, Train: 52.83%, Valid: 45.79%, Test: 27.53%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 24, Loss: 0.2604, Train: 42.00%, Valid: 35.96%, Test: 11.75%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 24, Loss: 0.2604, Train: 48.39%, Valid: 41.96%, Test: 20.98%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 24, Loss: 0.2604, Train: 53.76%, Valid: 46.92%, Test: 26.14%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 25, Loss: 0.2559, Train: 48.28%, Valid: 41.81%, Test: 23.48%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 25, Loss: 0.2559, Train: 54.89%, Valid: 47.98%, Test: 29.77%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 25, Loss: 0.2559, Train: 58.73%, Valid: 51.57%, Test: 36.38%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 26, Loss: 0.2524, Train: 43.09%, Valid: 36.69%, Test: 17.28%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 26, Loss: 0.2524, Train: 52.03%, Valid: 44.87%, Test: 25.31%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 26, Loss: 0.2524, Train: 54.66%, Valid: 47.30%, Test: 31.08%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 27, Loss: 0.2488, Train: 39.24%, Valid: 33.12%, Test: 18.45%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 27, Loss: 0.2488, Train: 50.03%, Valid: 42.99%, Test: 25.53%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 27, Loss: 0.2488, Train: 54.33%, Valid: 47.02%, Test: 31.94%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 28, Loss: 0.2455, Train: 42.31%, Valid: 35.80%, Test: 9.59%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 28, Loss: 0.2455, Train: 49.67%, Valid: 42.68%, Test: 20.23%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 28, Loss: 0.2455, Train: 55.71%, Valid: 48.32%, Test: 27.05%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 29, Loss: 0.2423, Train: 52.64%, Valid: 45.41%, Test: 22.23%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 29, Loss: 0.2423, Train: 57.18%, Valid: 49.67%, Test: 35.82%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 29, Loss: 0.2423, Train: 61.24%, Valid: 53.39%, Test: 50.47%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 30, Loss: 0.2382, Train: 45.00%, Valid: 38.28%, Test: 23.05%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 30, Loss: 0.2382, Train: 55.63%, Valid: 48.06%, Test: 28.63%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 30, Loss: 0.2382, Train: 59.33%, Valid: 51.50%, Test: 34.73%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 31, Loss: 0.2349, Train: 46.43%, Valid: 39.27%, Test: 22.58%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 31, Loss: 0.2349, Train: 55.12%, Valid: 47.42%, Test: 29.75%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 31, Loss: 0.2349, Train: 60.88%, Valid: 52.72%, Test: 34.81%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 32, Loss: 0.2323, Train: 49.33%, Valid: 42.17%, Test: 15.84%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 32, Loss: 0.2323, Train: 55.66%, Valid: 48.06%, Test: 27.96%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 32, Loss: 0.2323, Train: 60.45%, Valid: 52.54%, Test: 36.72%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 33, Loss: 0.2318, Train: 49.28%, Valid: 42.12%, Test: 18.68%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 33, Loss: 0.2318, Train: 56.10%, Valid: 48.29%, Test: 31.61%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 33, Loss: 0.2318, Train: 60.79%, Valid: 52.60%, Test: 38.52%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 34, Loss: 0.2300, Train: 55.20%, Valid: 47.59%, Test: 31.00%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 34, Loss: 0.2300, Train: 59.76%, Valid: 51.86%, Test: 38.99%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 34, Loss: 0.2300, Train: 62.46%, Valid: 54.38%, Test: 47.01%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 35, Loss: 0.2261, Train: 49.77%, Valid: 42.41%, Test: 21.73%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 35, Loss: 0.2261, Train: 57.85%, Valid: 49.85%, Test: 27.98%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 35, Loss: 0.2261, Train: 62.08%, Valid: 53.94%, Test: 36.06%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 36, Loss: 0.2239, Train: 52.75%, Valid: 45.07%, Test: 18.35%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 36, Loss: 0.2239, Train: 61.09%, Valid: 52.94%, Test: 30.93%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 36, Loss: 0.2239, Train: 64.42%, Valid: 56.01%, Test: 35.30%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 37, Loss: 0.2219, Train: 53.10%, Valid: 45.55%, Test: 28.06%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 37, Loss: 0.2219, Train: 60.55%, Valid: 52.42%, Test: 34.68%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 37, Loss: 0.2219, Train: 64.52%, Valid: 56.21%, Test: 38.06%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 38, Loss: 0.2198, Train: 59.44%, Valid: 51.36%, Test: 25.59%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 38, Loss: 0.2198, Train: 63.88%, Valid: 55.52%, Test: 36.81%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 38, Loss: 0.2198, Train: 66.22%, Valid: 57.72%, Test: 47.80%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 39, Loss: 0.2178, Train: 48.28%, Valid: 40.93%, Test: 9.24%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 39, Loss: 0.2178, Train: 58.62%, Valid: 50.60%, Test: 27.13%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 39, Loss: 0.2178, Train: 63.47%, Valid: 55.22%, Test: 35.06%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 40, Loss: 0.2184, Train: 55.25%, Valid: 47.17%, Test: 14.02%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 40, Loss: 0.2184, Train: 61.47%, Valid: 53.08%, Test: 29.73%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 40, Loss: 0.2184, Train: 65.94%, Valid: 57.36%, Test: 42.16%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 41, Loss: 0.2178, Train: 56.37%, Valid: 48.37%, Test: 28.11%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 41, Loss: 0.2178, Train: 63.83%, Valid: 55.35%, Test: 39.03%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 41, Loss: 0.2178, Train: 66.78%, Valid: 58.26%, Test: 45.13%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 42, Loss: 0.2144, Train: 56.31%, Valid: 48.24%, Test: 32.48%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 42, Loss: 0.2144, Train: 63.16%, Valid: 54.81%, Test: 38.92%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 42, Loss: 0.2144, Train: 66.68%, Valid: 58.18%, Test: 43.83%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 43, Loss: 0.2126, Train: 59.09%, Valid: 50.76%, Test: 26.23%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 43, Loss: 0.2126, Train: 64.99%, Valid: 56.44%, Test: 42.33%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 43, Loss: 0.2126, Train: 68.02%, Valid: 59.29%, Test: 49.81%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 44, Loss: 0.2110, Train: 62.31%, Valid: 54.09%, Test: 33.88%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 44, Loss: 0.2110, Train: 66.97%, Valid: 58.55%, Test: 49.96%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 44, Loss: 0.2110, Train: 69.05%, Valid: 60.52%, Test: 56.79%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 45, Loss: 0.2104, Train: 62.95%, Valid: 54.70%, Test: 34.71%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 45, Loss: 0.2104, Train: 68.11%, Valid: 59.56%, Test: 48.92%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 45, Loss: 0.2104, Train: 70.24%, Valid: 61.57%, Test: 55.06%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 46, Loss: 0.2099, Train: 49.45%, Valid: 41.71%, Test: 12.70%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 46, Loss: 0.2099, Train: 63.08%, Valid: 54.33%, Test: 30.67%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 46, Loss: 0.2099, Train: 67.63%, Valid: 58.66%, Test: 38.44%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 47, Loss: 0.2084, Train: 54.49%, Valid: 46.27%, Test: 13.31%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 47, Loss: 0.2084, Train: 63.53%, Valid: 54.82%, Test: 31.19%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 47, Loss: 0.2084, Train: 67.19%, Valid: 58.24%, Test: 44.22%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 48, Loss: 0.2056, Train: 59.06%, Valid: 50.65%, Test: 17.45%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 48, Loss: 0.2056, Train: 65.80%, Valid: 57.15%, Test: 41.00%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 48, Loss: 0.2056, Train: 69.83%, Valid: 61.02%, Test: 47.97%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 49, Loss: 0.2050, Train: 54.47%, Valid: 46.15%, Test: 20.93%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 49, Loss: 0.2050, Train: 63.66%, Valid: 54.68%, Test: 33.22%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 49, Loss: 0.2050, Train: 68.98%, Valid: 59.94%, Test: 45.48%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 50, Loss: 0.2042, Train: 58.72%, Valid: 50.14%, Test: 25.90%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 50, Loss: 0.2042, Train: 65.80%, Valid: 57.02%, Test: 38.59%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 50, Loss: 0.2042, Train: 69.19%, Valid: 60.24%, Test: 47.31%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 51, Loss: 0.2023, Train: 56.73%, Valid: 48.12%, Test: 10.24%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 51, Loss: 0.2023, Train: 64.19%, Valid: 55.44%, Test: 26.65%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 51, Loss: 0.2023, Train: 70.03%, Valid: 60.99%, Test: 38.64%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 52, Loss: 0.2017, Train: 60.49%, Valid: 51.48%, Test: 22.57%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 52, Loss: 0.2017, Train: 67.56%, Valid: 58.50%, Test: 47.69%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 52, Loss: 0.2017, Train: 70.73%, Valid: 61.63%, Test: 56.96%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 53, Loss: 0.2015, Train: 63.81%, Valid: 55.08%, Test: 37.30%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 53, Loss: 0.2015, Train: 69.08%, Valid: 60.09%, Test: 47.37%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 53, Loss: 0.2015, Train: 72.06%, Valid: 62.89%, Test: 56.53%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 54, Loss: 0.2012, Train: 59.67%, Valid: 50.90%, Test: 19.59%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 54, Loss: 0.2012, Train: 69.50%, Valid: 60.53%, Test: 41.89%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 54, Loss: 0.2012, Train: 72.62%, Valid: 63.50%, Test: 56.81%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 55, Loss: 0.1995, Train: 62.08%, Valid: 53.16%, Test: 16.41%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 55, Loss: 0.1995, Train: 68.98%, Valid: 59.89%, Test: 31.37%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 55, Loss: 0.1995, Train: 70.51%, Valid: 61.37%, Test: 42.55%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 56, Loss: 0.1980, Train: 59.60%, Valid: 50.90%, Test: 35.14%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 56, Loss: 0.1980, Train: 66.94%, Valid: 58.13%, Test: 47.83%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 56, Loss: 0.1980, Train: 69.98%, Valid: 60.97%, Test: 55.96%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 57, Loss: 0.1981, Train: 61.12%, Valid: 52.36%, Test: 21.10%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 57, Loss: 0.1981, Train: 67.25%, Valid: 58.27%, Test: 42.18%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 57, Loss: 0.1981, Train: 69.82%, Valid: 60.79%, Test: 58.84%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 58, Loss: 0.1952, Train: 49.97%, Valid: 41.61%, Test: 28.77%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 58, Loss: 0.1952, Train: 65.14%, Valid: 56.03%, Test: 42.96%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 58, Loss: 0.1952, Train: 70.15%, Valid: 60.93%, Test: 52.84%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 59, Loss: 0.1951, Train: 61.94%, Valid: 52.85%, Test: 21.65%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 59, Loss: 0.1951, Train: 66.92%, Valid: 57.65%, Test: 42.65%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 59, Loss: 0.1951, Train: 69.61%, Valid: 60.21%, Test: 48.41%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 60, Loss: 0.1947, Train: 60.77%, Valid: 51.98%, Test: 35.94%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 60, Loss: 0.1947, Train: 70.12%, Valid: 61.04%, Test: 50.24%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 60, Loss: 0.1947, Train: 71.84%, Valid: 62.64%, Test: 59.18%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 61, Loss: 0.1951, Train: 58.87%, Valid: 50.05%, Test: 32.53%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 61, Loss: 0.1951, Train: 69.42%, Valid: 60.24%, Test: 47.25%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 61, Loss: 0.1951, Train: 71.52%, Valid: 62.34%, Test: 54.05%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 62, Loss: 0.1930, Train: 62.12%, Valid: 53.06%, Test: 20.44%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 62, Loss: 0.1930, Train: 67.88%, Valid: 58.67%, Test: 42.09%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 62, Loss: 0.1930, Train: 71.17%, Valid: 61.89%, Test: 51.68%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 63, Loss: 0.1940, Train: 57.41%, Valid: 48.30%, Test: 26.76%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 63, Loss: 0.1940, Train: 64.57%, Valid: 55.12%, Test: 43.15%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 63, Loss: 0.1940, Train: 69.74%, Valid: 60.21%, Test: 52.73%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 64, Loss: 0.1918, Train: 62.67%, Valid: 53.55%, Test: 34.41%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 64, Loss: 0.1918, Train: 69.42%, Valid: 60.05%, Test: 46.37%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 64, Loss: 0.1918, Train: 71.00%, Valid: 61.60%, Test: 59.86%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 65, Loss: 0.1912, Train: 63.12%, Valid: 53.99%, Test: 32.53%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 65, Loss: 0.1912, Train: 66.70%, Valid: 57.54%, Test: 47.43%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 65, Loss: 0.1912, Train: 70.60%, Valid: 61.34%, Test: 53.99%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 66, Loss: 0.1906, Train: 62.20%, Valid: 53.12%, Test: 35.13%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 66, Loss: 0.1906, Train: 68.06%, Valid: 58.77%, Test: 54.71%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 66, Loss: 0.1906, Train: 71.51%, Valid: 62.16%, Test: 59.33%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 67, Loss: 0.1905, Train: 61.96%, Valid: 53.12%, Test: 26.40%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 67, Loss: 0.1905, Train: 71.14%, Valid: 62.03%, Test: 45.66%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 67, Loss: 0.1905, Train: 73.64%, Valid: 64.46%, Test: 60.10%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 68, Loss: 0.1893, Train: 56.92%, Valid: 47.41%, Test: 20.00%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 68, Loss: 0.1893, Train: 65.02%, Valid: 55.44%, Test: 34.69%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 68, Loss: 0.1893, Train: 69.79%, Valid: 60.33%, Test: 45.20%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 69, Loss: 0.1878, Train: 61.12%, Valid: 51.79%, Test: 34.37%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 69, Loss: 0.1878, Train: 69.77%, Valid: 60.35%, Test: 49.12%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 69, Loss: 0.1878, Train: 71.91%, Valid: 62.49%, Test: 63.81%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 70, Loss: 0.1889, Train: 59.74%, Valid: 50.54%, Test: 39.01%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 70, Loss: 0.1889, Train: 67.39%, Valid: 57.78%, Test: 46.25%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 70, Loss: 0.1889, Train: 72.24%, Valid: 62.51%, Test: 58.45%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 71, Loss: 0.1877, Train: 65.39%, Valid: 56.16%, Test: 36.46%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 71, Loss: 0.1877, Train: 72.24%, Valid: 62.78%, Test: 53.96%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 71, Loss: 0.1877, Train: 74.15%, Valid: 64.70%, Test: 62.19%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 72, Loss: 0.1879, Train: 63.28%, Valid: 54.05%, Test: 43.16%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 72, Loss: 0.1879, Train: 67.75%, Valid: 58.48%, Test: 55.95%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 72, Loss: 0.1879, Train: 73.28%, Valid: 63.82%, Test: 64.64%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 73, Loss: 0.1856, Train: 62.47%, Valid: 53.12%, Test: 38.20%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 73, Loss: 0.1856, Train: 66.23%, Valid: 56.89%, Test: 49.70%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 73, Loss: 0.1856, Train: 71.59%, Valid: 62.12%, Test: 58.17%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 74, Loss: 0.1850, Train: 60.38%, Valid: 51.14%, Test: 30.73%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 74, Loss: 0.1850, Train: 68.21%, Valid: 58.68%, Test: 43.87%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 74, Loss: 0.1850, Train: 73.26%, Valid: 63.73%, Test: 60.81%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 75, Loss: 0.1850, Train: 63.99%, Valid: 54.65%, Test: 43.00%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 75, Loss: 0.1850, Train: 70.12%, Valid: 60.65%, Test: 58.45%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 75, Loss: 0.1850, Train: 72.12%, Valid: 62.62%, Test: 63.38%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 76, Loss: 0.1842, Train: 57.91%, Valid: 48.77%, Test: 17.22%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 76, Loss: 0.1842, Train: 69.65%, Valid: 60.27%, Test: 35.25%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 76, Loss: 0.1842, Train: 74.09%, Valid: 64.69%, Test: 50.94%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 77, Loss: 0.1835, Train: 65.36%, Valid: 56.09%, Test: 37.02%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 77, Loss: 0.1835, Train: 72.70%, Valid: 63.31%, Test: 55.85%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 77, Loss: 0.1835, Train: 74.56%, Valid: 65.24%, Test: 71.47%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 78, Loss: 0.1845, Train: 60.31%, Valid: 51.01%, Test: 28.91%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 78, Loss: 0.1845, Train: 70.85%, Valid: 61.42%, Test: 45.47%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 78, Loss: 0.1845, Train: 75.40%, Valid: 65.91%, Test: 57.55%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 79, Loss: 0.1830, Train: 66.77%, Valid: 57.56%, Test: 39.89%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 79, Loss: 0.1830, Train: 70.23%, Valid: 60.95%, Test: 51.51%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 79, Loss: 0.1830, Train: 72.78%, Valid: 63.32%, Test: 62.35%\n",
      "---\n",
      "Hits@10\n",
      "Run: 01, Epoch: 80, Loss: 0.1836, Train: 64.87%, Valid: 55.53%, Test: 48.03%\n",
      "Hits@20\n",
      "Run: 01, Epoch: 80, Loss: 0.1836, Train: 69.98%, Valid: 60.49%, Test: 58.86%\n",
      "Hits@30\n",
      "Run: 01, Epoch: 80, Loss: 0.1836, Train: 73.65%, Valid: 64.02%, Test: 66.09%\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "loggers = repeat_experiments(\n",
    "    g, emb, model, predictor, split_idx, \n",
    "    device, train_args, N_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1189b96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@10\n",
      "All runs:\n",
      "Highest Train: 73.77 ± 0.94\n",
      "Highest Valid: 63.44 ± 0.97\n",
      "  Final Train: 73.75 ± 0.95\n",
      "   Final Test: 35.28 ± 11.36\n",
      "Hits@20\n",
      "All runs:\n",
      "Highest Train: 78.22 ± 0.48\n",
      "Highest Valid: 67.97 ± 0.47\n",
      "  Final Train: 78.16 ± 0.52\n",
      "   Final Test: 56.20 ± 12.88\n",
      "Hits@30\n",
      "All runs:\n",
      "Highest Train: 79.92 ± 0.35\n",
      "Highest Valid: 69.70 ± 0.32\n",
      "  Final Train: 79.91 ± 0.35\n",
      "   Final Test: 72.48 ± 5.89\n"
     ]
    }
   ],
   "source": [
    "# Final performance\n",
    "for key in loggers.keys():\n",
    "    print(key)\n",
    "    loggers[key].print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ddcf2fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This comparison only makes sense if using ~200 epochs during training\n",
    "norm_plot([\n",
    "    (test_hits_lb, test_hits_lb_std, \"GraphSAGE L.B.\"),\n",
    "    (0.5620, 0.1288, 'GraphSAGE local'),\n",
    "], 'Test Hits@20 Comparison over 10 runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96194dc3",
   "metadata": {},
   "source": [
    "In my experiments, I observe an average performance that's slightly above the leaderboard's, but with a standard deviation that's 3x as large.  \n",
    "\n",
    "# Improvements with Jumping Knowledge connections\n",
    "In this section we implement [Jumping Knowledge](https://arxiv.org/abs/1806.03536) connections, which allow the model to jointly consider information from different distances/hops from a node.  This is accomplished by aggregating all the intermedite representations of the GNN layers.  Before, we got a node's embedding for the downstream Link Predictor model by taking the output of the *last GNN layer*.  Now, we will directly aggregate the embeddings from *each GNN layer* and use this result as a node's final embedding.  This has a strong impact on the results for this task.\n",
    "\n",
    "The original work presents a number of different ways of combining embeddings from multiple layers.  In our case, we will simply concatenate the embeddings of a node from each layer and then apply a linear projection:\n",
    "\n",
    "$$ \\mathbf{h}_i^{out} = \\mathbf{W}^{out} \\text{concat} \\left( [ \\mathbf{h}_i^0, \\ldots, \\mathbf{h}_i^K ] \\right)$$\n",
    "\n",
    "where $\\mathbf{W}^{out}$ is a learnable projection matrix and $K$ is the number of GNN layers.\n",
    "\n",
    "We will still use the SAGE layer created earlier in this project, but we will change how they are combined.  To get started, I have copy/pasted the `SAGE` model code we used above from the `utils.py` file and merely changed the class name.  Your task will be to modify the logic to include Jumping Knowledge connections and measure the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "268f43ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JKSAGE(nn.Module):\n",
    "    \"\"\"\n",
    "    This class is copy/pasted from the SAGE class and needs to be modified\n",
    "    to include Jumping Knowledge connections\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, sage_cls):\n",
    "        super(JKSAGE, self).__init__()\n",
    "        # TODO\n",
    "        \"\"\"\n",
    "        Will need to add the linear projection.  Do not use a bias term\n",
    "        for this final linear projection.\n",
    "        \n",
    "        \n",
    "        Arguments\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of input features to the layer\n",
    "        hidden_channels : int\n",
    "            Number of channels in the hidden layers\n",
    "        out_channels : int\n",
    "            Number of desired output features from the layer\n",
    "        num_layers : int\n",
    "            The number of GNN layers\n",
    "        dropout : float\n",
    "            The dropout rate to apply to the hidden layers\n",
    "        sage_cls : nn.Module\n",
    "            The torch Module object that implements GraphSAGE convolution\n",
    "        \"\"\"\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(sage_cls(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(sage_cls(hidden_channels, hidden_channels))\n",
    "        self.convs.append(sage_cls(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # TODO\n",
    "        \"\"\"\n",
    "        Modify to include the new linear layer parameters\n",
    "        \"\"\"\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "            \n",
    "    @property\n",
    "    def num_layers(self):\n",
    "        return len(self.convs)\n",
    "    \n",
    "    def forward(self, gs, x):\n",
    "        # TODO\n",
    "        \"\"\"\n",
    "        The forward pass of the model, which needs to be modified to \n",
    "        include Jumping Knowledge connections.\n",
    "        \n",
    "        Note:  *Each* GraphSAGE layer should have this flow:\n",
    "        \n",
    "            input -> GraphSAGE -> ReLU -> Dropout\n",
    "            \n",
    "        This output of each layer will be used for concatenation.\n",
    "        \n",
    "        \n",
    "        The final output of this method should be a linear projection \n",
    "        of the concatenated set of GNN outputs.  The result of this \n",
    "        final linear projection should be returned directly--no activation \n",
    "        or dropout should be applied.\n",
    "        \n",
    "        Arguments\n",
    "        ----------\n",
    "        gs : DGLGraph or list<DGLBlock>\n",
    "            The graph or minibatch list of blocks used for message passing\n",
    "        x : Tensor\n",
    "            The node features\n",
    "        \"\"\"\n",
    "        h = x\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            if gs.is_block:\n",
    "                g = gs[i]\n",
    "            else:\n",
    "                g = gs  # full graph\n",
    "            h = conv(g, h)\n",
    "            if i != len(self.convs) - 1:\n",
    "                h = F.relu(h)\n",
    "                h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1086f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_jk = JKSAGE(\n",
    "    num_hidden, num_hidden, num_hidden, num_layers,\n",
    "    dropout, sage_cls=MySage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f13d7cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(p.numel() for p in model_jk.parameters()) == 393728, \"Number of JKSAGE parameters doesn't match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1598b8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loggers_jk = repeat_experiments(\n",
    "    g, emb, model_jk, predictor, split_idx, \n",
    "    device, train_args, N_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4c7d1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@10\n",
      "All runs:\n",
      "Highest Train: 72.90 ± 1.19\n",
      "Highest Valid: 63.51 ± 1.14\n",
      "  Final Train: 72.89 ± 1.20\n",
      "   Final Test: 22.91 ± 11.00\n",
      "Hits@20\n",
      "All runs:\n",
      "Highest Train: 77.43 ± 0.57\n",
      "Highest Valid: 67.92 ± 0.52\n",
      "  Final Train: 77.42 ± 0.58\n",
      "   Final Test: 67.11 ± 5.36\n",
      "Hits@30\n",
      "All runs:\n",
      "Highest Train: 78.89 ± 0.33\n",
      "Highest Valid: 69.36 ± 0.32\n",
      "  Final Train: 78.88 ± 0.33\n",
      "   Final Test: 75.47 ± 4.75\n"
     ]
    }
   ],
   "source": [
    "for key in loggers_jk.keys():\n",
    "    print(key)\n",
    "    loggers_jk[key].print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "607c2f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This comparison only makes sense if using ~200 epochs during training\n",
    "norm_plot([\n",
    "    (test_hits_lb, test_hits_lb_std, \"GraphSAGE L.B.\"),\n",
    "    (0.5620, 0.1288, 'GraphSAGE local'),\n",
    "    (.6711, .0536, 'GraphSAGE+JK')\n",
    "], 'Test Hits@20 Comparison over 10 runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8917222",
   "metadata": {},
   "source": [
    "My experiments show a 25% relative performance improvement and a significantly reduced variation compared to my experiments without JK connections.\n",
    "\n",
    "# Extra Credit\n",
    "In general, it's a great use of time to find a submission on the leaderboard that's interesting and figure out how to implement it.  Here are two possible avenues for additional exploration:\n",
    "1. In our Jumping Knowledge connections, instead of concatenating the embeddings of each layer and applying a linear projection to the result, we might instead aggregate our GNN-layer representations by applying an attention mechanism or an RNN, like an LSTM.  \n",
    "2. We might extend our graph with high-confidence edges, as is proposed in the [Edge Proposal Set method](https://arxiv.org/abs/2106.15810).  In it, they essentially use a model to find the high-confidence edges (\"Filter\"), and then add those to the graph before running the actual Link Prediction task (\"Rank\").  In essence, they use a two-step approach in which the first step is pre-processing the graph to find new edges, and the second step runs as we have here, but now with a modified graph topology that will ideally improve performance.  This can be considered a form of \"graph rewiring\", in which the original graph is modified to facilitate better performance on a task.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
